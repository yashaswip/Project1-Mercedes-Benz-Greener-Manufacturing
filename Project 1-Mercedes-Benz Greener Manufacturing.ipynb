{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64477f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('/Users/yashaswipatki/Downloads/train.csv')  \n",
    "test_df = pd.read_csv('/Users/yashaswipatki/Downloads/test.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47bca019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data after removing zero variance columns:\n",
      "    ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...  X375  X376  X377  X378  X379  \\\n",
      "0   0  130.81   k  v  at  a  d  u  j  o  ...     0     0     1     0     0   \n",
      "1   6   88.53   k  t  av  e  d  y  l  o  ...     1     0     0     0     0   \n",
      "2   7   76.26  az  w   n  c  d  x  j  x  ...     0     0     0     0     0   \n",
      "3   9   80.62  az  t   n  f  d  x  l  e  ...     0     0     0     0     0   \n",
      "4  13   78.02  az  v   n  f  d  h  d  n  ...     0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 366 columns]\n",
      "\n",
      "Test Data after removing zero variance columns:\n",
      "    ID  X0 X1  X2 X3 X4 X5 X6 X8  X10  ...  X375  X376  X377  X378  X379  X380  \\\n",
      "0   1  az  v   n  f  d  t  a  w    0  ...     0     0     0     1     0     0   \n",
      "1   2   t  b  ai  a  d  b  g  y    0  ...     0     0     1     0     0     0   \n",
      "2   3  az  v  as  f  d  a  j  j    0  ...     0     0     0     1     0     0   \n",
      "3   4  az  l   n  f  d  z  l  n    0  ...     0     0     0     1     0     0   \n",
      "4   5   w  s  as  c  d  y  i  m    0  ...     1     0     0     0     0     0   \n",
      "\n",
      "   X382  X383  X384  X385  \n",
      "0     0     0     0     0  \n",
      "1     0     0     0     0  \n",
      "2     0     0     0     0  \n",
      "3     0     0     0     0  \n",
      "4     0     0     0     0  \n",
      "\n",
      "[5 rows x 365 columns]\n"
     ]
    }
   ],
   "source": [
    "# Identify and remove columns with zero variance\n",
    "zero_variance_cols = train_df.columns[train_df.apply(pd.Series.nunique) == 1]\n",
    "train_df = train_df.drop(zero_variance_cols, axis=1)\n",
    "test_df = test_df.drop(zero_variance_cols, axis=1)\n",
    "\n",
    "# Display the updated datasets\n",
    "print(\"Train Data after removing zero variance columns:\\n\", train_df.head())\n",
    "print(\"\\nTest Data after removing zero variance columns:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfe8b7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data - Null Values:\n",
      " ID      0\n",
      "y       0\n",
      "X0      0\n",
      "X1      0\n",
      "X2      0\n",
      "       ..\n",
      "X380    0\n",
      "X382    0\n",
      "X383    0\n",
      "X384    0\n",
      "X385    0\n",
      "Length: 366, dtype: int64\n",
      "\n",
      "Test Data - Null Values:\n",
      " ID      0\n",
      "X0      0\n",
      "X1      0\n",
      "X2      0\n",
      "X3      0\n",
      "       ..\n",
      "X380    0\n",
      "X382    0\n",
      "X383    0\n",
      "X384    0\n",
      "X385    0\n",
      "Length: 365, dtype: int64\n",
      "\n",
      "Train Data - Unique Values:\n",
      " ID      4209\n",
      "y       2545\n",
      "X0        47\n",
      "X1        27\n",
      "X2        44\n",
      "        ... \n",
      "X380       2\n",
      "X382       2\n",
      "X383       2\n",
      "X384       2\n",
      "X385       2\n",
      "Length: 366, dtype: int64\n",
      "\n",
      "Test Data - Unique Values:\n",
      " ID      4209\n",
      "X0        49\n",
      "X1        27\n",
      "X2        45\n",
      "X3         7\n",
      "        ... \n",
      "X380       2\n",
      "X382       2\n",
      "X383       2\n",
      "X384       2\n",
      "X385       2\n",
      "Length: 365, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(\"Train Data - Null Values:\\n\", train_df.isnull().sum())\n",
    "print(\"\\nTest Data - Null Values:\\n\", test_df.isnull().sum())\n",
    "\n",
    "# Check for unique values\n",
    "print(\"\\nTrain Data - Unique Values:\\n\", train_df.nunique())\n",
    "print(\"\\nTest Data - Unique Values:\\n\", test_df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c8f6e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data after label encoding:\n",
      "    ID       y  X0  X1  X2  X3  X4  X5  X6  X8  ...  X375  X376  X377  X378  \\\n",
      "0   0  130.81  37  23  20   0   3  27   9  14  ...     0     0     1     0   \n",
      "1   6   88.53  37  21  22   4   3  31  11  14  ...     1     0     0     0   \n",
      "2   7   76.26  24  24  38   2   3  30   9  23  ...     0     0     0     0   \n",
      "3   9   80.62  24  21  38   5   3  30  11   4  ...     0     0     0     0   \n",
      "4  13   78.02  24  23  38   5   3  14   3  13  ...     0     0     0     0   \n",
      "\n",
      "   X379  X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0  \n",
      "2     0     0     1     0     0     0  \n",
      "3     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 366 columns]\n",
      "\n",
      "Test Data after label encoding:\n",
      "    ID   y  X0  X1  X2  X3  X4  X5  X6  X8  ...  X375  X376  X377  X378  X379  \\\n",
      "0   1 NaN  24  23  38   5   3  26   0  22  ...     0     0     0     1     0   \n",
      "1   2 NaN  46   3   9   0   3   9   6  24  ...     0     0     1     0     0   \n",
      "2   3 NaN  24  23  19   5   3   0   9   9  ...     0     0     0     1     0   \n",
      "3   4 NaN  24  13  38   5   3  32  11  13  ...     0     0     0     1     0   \n",
      "4   5 NaN  49  20  19   2   3  31   8  12  ...     1     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     0     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 366 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine train and test data for label encoding\n",
    "combined_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# Identify categorical columns for label encoding\n",
    "categorical_cols = combined_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert all values to strings before label encoding\n",
    "combined_df[categorical_cols] = combined_df[categorical_cols].astype(str)\n",
    "\n",
    "# Apply label encoder to combined data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    combined_df[col] = label_encoder.fit_transform(combined_df[col])\n",
    "\n",
    "# Split the combined data back into train and test sets\n",
    "train_df = combined_df[:len(train_df)]\n",
    "test_df = combined_df[len(train_df):]\n",
    "\n",
    "# Display the updated datasets\n",
    "print(\"Train Data after label encoding:\\n\", train_df.head())\n",
    "print(\"\\nTest Data after label encoding:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae1e2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data after dimensionality reduction:\n",
      "        PCA_1      PCA_2     PCA_3     PCA_4     PCA_5     PCA_6     PCA_7  \\\n",
      "0  12.355577  -2.931375 -0.964477  1.871028 -1.131071 -3.802995  9.041632   \n",
      "1  -0.146158   0.443786  0.900259  1.328665 -2.575386 -0.199335  1.010269   \n",
      "2   9.911795  21.433722 -4.588380 -4.588246  0.611327  2.658488  1.143245   \n",
      "3   6.999444  21.646649 -5.535007 -0.071694  2.115799  0.657452  0.412677   \n",
      "4   6.203074  21.740216 -6.092918  0.606826  2.195153 -0.074753 -0.993929   \n",
      "\n",
      "      PCA_8      PCA_9    PCA_10  ...    PCA_141   PCA_142   PCA_143  \\\n",
      "0 -3.593319 -16.149030  8.407628  ...  -0.112764 -0.246420  0.056195   \n",
      "1 -0.592836  -0.486332  0.144286  ...   0.987769 -0.322127 -0.975282   \n",
      "2  3.642861  -0.919221  1.314535  ... -11.603568 -7.603832  0.683216   \n",
      "3 -0.036918   0.164190  2.929633  ...   1.986847  0.871403  0.650220   \n",
      "4 -0.231459   0.873334  0.814194  ... -12.266493 -8.932799  2.265767   \n",
      "\n",
      "    PCA_144   PCA_145   PCA_146   PCA_147   PCA_148   PCA_149   PCA_150  \n",
      "0 -0.406046 -0.070564  0.044060 -0.268959 -0.021462 -0.373475 -0.229598  \n",
      "1  1.013545 -0.925752 -1.275597 -1.387641  2.176028  2.969996  0.368898  \n",
      "2  0.616658  0.178882  3.504609  2.259561  1.146718  0.916193  0.167351  \n",
      "3  0.008626 -0.251331  0.458748 -0.386268 -0.125489 -0.478741  0.554559  \n",
      "4  1.358300  0.377344  6.779990  3.397818  2.217386  1.993318  1.347357  \n",
      "\n",
      "[5 rows x 150 columns]\n",
      "\n",
      "Test Data after dimensionality reduction:\n",
      " Empty DataFrame\n",
      "Columns: [PCA_1, PCA_2, PCA_3, PCA_4, PCA_5, PCA_6, PCA_7, PCA_8, PCA_9, PCA_10, PCA_11, PCA_12, PCA_13, PCA_14, PCA_15, PCA_16, PCA_17, PCA_18, PCA_19, PCA_20, PCA_21, PCA_22, PCA_23, PCA_24, PCA_25, PCA_26, PCA_27, PCA_28, PCA_29, PCA_30, PCA_31, PCA_32, PCA_33, PCA_34, PCA_35, PCA_36, PCA_37, PCA_38, PCA_39, PCA_40, PCA_41, PCA_42, PCA_43, PCA_44, PCA_45, PCA_46, PCA_47, PCA_48, PCA_49, PCA_50, PCA_51, PCA_52, PCA_53, PCA_54, PCA_55, PCA_56, PCA_57, PCA_58, PCA_59, PCA_60, PCA_61, PCA_62, PCA_63, PCA_64, PCA_65, PCA_66, PCA_67, PCA_68, PCA_69, PCA_70, PCA_71, PCA_72, PCA_73, PCA_74, PCA_75, PCA_76, PCA_77, PCA_78, PCA_79, PCA_80, PCA_81, PCA_82, PCA_83, PCA_84, PCA_85, PCA_86, PCA_87, PCA_88, PCA_89, PCA_90, PCA_91, PCA_92, PCA_93, PCA_94, PCA_95, PCA_96, PCA_97, PCA_98, PCA_99, PCA_100, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 150 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine train and test data for dimensionality reduction\n",
    "combined_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# Drop rows with missing values\n",
    "combined_df.dropna(inplace=True)\n",
    "\n",
    "# Identify features\n",
    "features = combined_df\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "features_pca = pca.fit_transform(features_standardized)\n",
    "\n",
    "# Split the combined data back into train and test sets\n",
    "train_df_pca = pd.DataFrame(features_pca[:len(train_df)], columns=[f'PCA_{i+1}' for i in range(features_pca.shape[1])])\n",
    "test_df_pca = pd.DataFrame(features_pca[len(train_df):], columns=[f'PCA_{i+1}' for i in range(features_pca.shape[1])])\n",
    "\n",
    "# Display the updated datasets\n",
    "print(\"Train Data after dimensionality reduction:\\n\", train_df_pca.head())\n",
    "print(\"\\nTest Data after dimensionality reduction:\\n\", test_df_pca.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f762b046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/yashaswipatki/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Users/yashaswipatki/anaconda3/lib/python3.11/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/yashaswipatki/anaconda3/lib/python3.11/site-packages (from xgboost) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a872a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'y', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8',\n",
      "       ...\n",
      "       'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X382', 'X383', 'X384',\n",
      "       'X385'],\n",
      "      dtype='object', length=366)\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "858f1726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Validation Set: 57.32378750579196\n",
      "Number of rows in test_df_pca: 0\n",
      "Predicted Values for test_df:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have already performed the previous steps including dimensionality reduction\n",
    "\n",
    "# Define the target variable\n",
    "target_column = 'y'  # Replace with the actual target column name\n",
    "target = combined_df[target_column]\n",
    "\n",
    "# Split the train_df_pca into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df_pca, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost regressor\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set (you can use different metrics depending on your task)\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "print(f'Mean Squared Error on Validation Set: {mse}')\n",
    "\n",
    "# Now, predict the test_df values\n",
    "print(\"Number of rows in test_df_pca:\", test_df_pca.shape[0])  # Print the number of rows in test_df_pca\n",
    "test_predictions = xgb_reg.predict(test_df_pca)\n",
    "\n",
    "# Check if test_predictions is empty or contains NaN values\n",
    "if test_predictions is None or any(np.isnan(test_predictions)):\n",
    "    print(\"Test predictions are empty or contain NaN values.\")\n",
    "else:\n",
    "    # Display the predicted values for the test_df\n",
    "    print(\"Predicted Values for test_df:\\n\", test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5910ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
